{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cae.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nzkOXMbGov_"
      },
      "source": [
        "# smooth.py\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from skimage.io import imsave\n",
        "\n",
        "\n",
        "def lin_interp(n, p1, p2):\n",
        "    x = np.zeros((n, p1.shape[0], 128, 3))\n",
        "\n",
        "    for i in range(n):\n",
        "        a = (i + 1) / (n + 1)\n",
        "        x[i] = (1 - a) * p1 + a * p2\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def smooth(in_img, ws):\n",
        "    _name, _ext = os.path.splitext(in_img)\n",
        "    out_img = f\"{_name}_s{ws}{_ext}\"\n",
        "\n",
        "    in_img = np.array(Image.open(in_img)) / 255.0\n",
        "    orig_img = in_img[24:-24, :1280, :]  # left image, remove borders\n",
        "    in_img = in_img[:, 1280:, :]  # right image\n",
        "\n",
        "    # 6,10,128,128,3\n",
        "    patches = np.reshape(in_img, (6, 128, 10, 128, 3))\n",
        "    patches = np.transpose(patches, (0, 2, 1, 3, 4))\n",
        "\n",
        "    h = ws // 2\n",
        "\n",
        "    for i in range(5):\n",
        "        p1 = patches[i, :, 128 - h, :, :]\n",
        "        p2 = patches[i + 1, :, h, :, :]\n",
        "\n",
        "        x = lin_interp(ws, p1, p2)\n",
        "        patches[i, :, 128 - h :, :, :] = np.transpose(x[:h, :, :, :], (1, 0, 2, 3))\n",
        "        patches[i + 1, :, :h, :, :] = np.transpose(x[h:, :, :, :], (1, 0, 2, 3))\n",
        "\n",
        "    for j in range(9):\n",
        "        p3 = patches[:, j, :, 128 - h, :]\n",
        "        p4 = patches[:, j + 1, :, h, :]\n",
        "\n",
        "        x = lin_interp(ws, p3, p4)\n",
        "        patches[:, j, :, 128 - h :, :] = np.transpose(x[:h, :, :, :], (1, 2, 0, 3))\n",
        "        patches[:, j + 1, :, :h, :] = np.transpose(x[h:, :, :, :], (1, 2, 0, 3))\n",
        "\n",
        "    out = np.transpose(patches, (0, 2, 1, 3, 4))\n",
        "    out = np.reshape(out, (768, 1280, 3))\n",
        "    out = out[24:-24, :, :]\n",
        "\n",
        "    out = np.concatenate((orig_img, out), axis=1)\n",
        "    imsave(out_img, out)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     parser = argparse.ArgumentParser()\n",
        "#     parser.add_argument(\"--in_img\", type=str, required=True)\n",
        "#     parser.add_argument(\"--window_size\", type=int, required=True)\n",
        "#     args = parser.parse_args()\n",
        "\n",
        "#     # make sure an even size is used\n",
        "#     args.window_size += args.window_size % 2\n",
        "\n",
        "#     smooth(args.in_img, args.window_size)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4VJSqjcX5hV",
        "outputId": "c389f553-d482-46ea-f039-8565eeed4008"
      },
      "source": [
        "!pip install colored\n",
        "!pip install numpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting colored\n",
            "  Downloading colored-1.4.3.tar.gz (29 kB)\n",
            "Building wheels for collected packages: colored\n",
            "  Building wheel for colored (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for colored: filename=colored-1.4.3-py3-none-any.whl size=14341 sha256=dbd35bf2255dfaff88e6fa4b2208fe78566323f1dd476120efe180cd48460761\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/f6/00/835e81851bc345428a253721c8bdad0062721dfb861bc6e752\n",
            "Successfully built colored\n",
            "Installing collected packages: colored\n",
            "Successfully installed colored-1.4.3\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQXviw2-RJHs"
      },
      "source": [
        "# logger.py\n",
        "\"\"\"\n",
        "Inspired by https://github.com/SebiSebi/friendlylog\n",
        "\"\"\"\n",
        "\n",
        "import logging\n",
        "import sys\n",
        "from copy import copy\n",
        "from typing import Union\n",
        "\n",
        "from colored import attr, fg\n",
        "\n",
        "DEBUG = \"debug\"\n",
        "INFO = \"info\"\n",
        "WARNING = \"warning\"\n",
        "ERROR = \"error\"\n",
        "CRITICAL = \"critical\"\n",
        "\n",
        "LOG_LEVELS = {\n",
        "    DEBUG: logging.DEBUG,\n",
        "    INFO: logging.INFO,\n",
        "    WARNING: logging.WARNING,\n",
        "    ERROR: logging.ERROR,\n",
        "    CRITICAL: logging.CRITICAL,\n",
        "}\n",
        "\n",
        "\n",
        "class _Formatter(logging.Formatter):\n",
        "    def __init__(self, colorize=False, *args, **kwargs):\n",
        "        super(_Formatter, self).__init__(*args, **kwargs)\n",
        "        self.colorize = colorize\n",
        "\n",
        "    @staticmethod\n",
        "    def _process(msg, loglevel, colorize):\n",
        "        loglevel = str(loglevel).lower()\n",
        "        if loglevel not in LOG_LEVELS:\n",
        "            raise RuntimeError(\n",
        "                f\"{loglevel} should be one of {LOG_LEVELS}.\"\n",
        "            )  # pragma: no cover\n",
        "\n",
        "        msg = f\"{str(loglevel).upper()}: {str(msg)}\"\n",
        "\n",
        "        if not colorize:\n",
        "            return msg\n",
        "\n",
        "        if loglevel == DEBUG:\n",
        "            return \"{}{}{}\".format(fg(5), msg, attr(0))  # noqa: E501\n",
        "        if loglevel == INFO:\n",
        "            return \"{}{}{}\".format(fg(4), msg, attr(0))  # noqa: E501\n",
        "        if loglevel == WARNING:\n",
        "            return \"{}{}{}{}{}\".format(\n",
        "                fg(214), attr(1), msg, attr(21), attr(0)\n",
        "            )  # noqa: E501\n",
        "        if loglevel == ERROR:\n",
        "            return \"{}{}{}{}{}\".format(\n",
        "                fg(202), attr(1), msg, attr(21), attr(0)\n",
        "            )  # noqa: E501\n",
        "        if loglevel == CRITICAL:\n",
        "            return \"{}{}{}{}{}\".format(\n",
        "                fg(196), attr(1), msg, attr(21), attr(0)\n",
        "            )  # noqa: E501\n",
        "\n",
        "    def format(self, record):\n",
        "        record = copy(record)\n",
        "        loglevel = record.levelname\n",
        "        record.msg = _Formatter._process(record.msg, loglevel, self.colorize)\n",
        "        return super(_Formatter, self).format(record)\n",
        "\n",
        "\n",
        "class Logger:\n",
        "    def __init__(self, name=\"default\", colorize=False, stream=sys.stdout, level=DEBUG):\n",
        "        self.name = name\n",
        "\n",
        "        # get the logger object; keep it hidden as there's no need to directly access it\n",
        "        self.__logger = logging.getLogger(f\"_logger-{name}\")\n",
        "        self.__logger.propagate = False\n",
        "        self.setLevel(level.lower())\n",
        "\n",
        "        # use the custom formatter\n",
        "        self.__formatter = _Formatter(\n",
        "            colorize=colorize,\n",
        "            fmt=\"[%(process)d][%(asctime)s.%(msecs)03d @ %(funcName)s] %(message)s\",\n",
        "            datefmt=\"%y-%m-%d %H:%M:%S\",\n",
        "        )\n",
        "\n",
        "        # install default handler\n",
        "        self.__stream_to_handler = {}\n",
        "        self.clear_handlers()\n",
        "        self.__main_handler = self.add_handler(stream)\n",
        "\n",
        "        # install logging functions\n",
        "        self.debug = self.__logger.debug\n",
        "        self.info = self.__logger.info\n",
        "        self.warning = self.__logger.warning\n",
        "        self.error = self.__logger.error\n",
        "        self.critical = self.__logger.critical\n",
        "\n",
        "    def log_function(self):\n",
        "        def wrapper(func):\n",
        "            def func_wrapper(*args, **kwargs):\n",
        "                self.__logger.info(\n",
        "                    f\"calling <{func.__name__}>\\n\\t  args: {args}\\n\\tkwargs: {kwargs}\"\n",
        "                )\n",
        "                out = func(*args, **kwargs)\n",
        "                self.__logger.info(f\"exiting <{func.__name__}>\")\n",
        "                return out\n",
        "\n",
        "            return func_wrapper\n",
        "\n",
        "        return wrapper\n",
        "\n",
        "    def setLevel(self, level: Union[str, int]) -> None:\n",
        "        if isinstance(level, int):\n",
        "            self.__logger.setLevel(level)\n",
        "        else:\n",
        "            if level.lower() not in LOG_LEVELS:\n",
        "                raise ValueError(f\"level should be one of {LOG_LEVELS}\")\n",
        "            self.__logger.setLevel(LOG_LEVELS[level.lower()])\n",
        "\n",
        "    def add_handler(self, stream) -> logging.StreamHandler:\n",
        "        handler = logging.StreamHandler(stream)\n",
        "        handler.setFormatter(self.__formatter)\n",
        "        self.__logger.addHandler(handler)\n",
        "        self.__stream_to_handler[stream] = handler\n",
        "        return handler\n",
        "\n",
        "    def remove_handler(self, stream) -> bool:\n",
        "        if stream in self.__stream_to_handler:\n",
        "            self.__logger.removeHandler(self.__stream_to_handler[stream])\n",
        "            self.__stream_to_handler.pop(stream)\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def clear_handlers(self) -> None:\n",
        "        self.__logger.handlers = []\n",
        "        self.__stream_to_handler = {}\n",
        "\n",
        "    def get_handlers(self) -> list:\n",
        "        return self.__logger.handlers\n",
        "\n",
        "    # Don't use these unless you know what you are doing\n",
        "\n",
        "    @property\n",
        "    def inner_logger(self):\n",
        "        return self.__logger\n",
        "\n",
        "    @property\n",
        "    def inner_stream_handler(self):\n",
        "        return self.__main_handler\n",
        "\n",
        "    @property\n",
        "    def inner_formatter(self):\n",
        "        return self.__formatter\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgPZTrYORed0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "825b9cd9-627f-469a-b257-97637e81376b"
      },
      "source": [
        "# resize.sh dont need to run first\n",
        "#!/bin/bash\n",
        "\n",
        "file_name=$(basename \"$1\"); file_name=\"${file_name%.*}\"\n",
        "dir_name=$(dirname \"$1\")\n",
        "\n",
        "convert $1 -resize 1280x720! ${dir_name}/${file_name}.bmp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-07217a02ce54>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    file_name=$(basename \"$1\"); file_name=\"${file_name%.*}\"\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtG9QVF-RkdE"
      },
      "source": [
        "# utils.py\n",
        "import struct\n",
        "import numpy as np\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "\n",
        "def save_imgs(imgs, to_size, name) -> None:\n",
        "    # x = np.array(x)\n",
        "    # x = np.transpose(x, (1, 2, 0)) * 255\n",
        "    # x = x.astype(np.uint8)\n",
        "    # imsave(name, x)\n",
        "\n",
        "    # x = 0.5 * (x + 1)\n",
        "\n",
        "    # to_size = (C, H, W)\n",
        "    imgs = imgs.clamp(0, 1)\n",
        "    imgs = imgs.view(imgs.size(0), *to_size)\n",
        "    save_image(imgs, name)\n",
        "\n",
        "\n",
        "def save_encoded(enc: np.ndarray, fname: str) -> None:\n",
        "    enc = np.reshape(enc, -1)\n",
        "    sz = str(len(enc)) + \"d\"\n",
        "\n",
        "    with open(fname, \"wb\") as fp:\n",
        "        fp.write(struct.pack(sz, *enc))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy8t9ilxRwgk"
      },
      "source": [
        "# data_loader.py\n",
        "from pathlib import Path\n",
        "from typing import Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch as T\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class ImageFolder720p(Dataset):\n",
        "    \"\"\"\n",
        "    Image shape is (720, 1280, 3) --> (768, 1280, 3) --> 6x10 128x128 patches\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root: str):\n",
        "        self.files = sorted(Path(root).iterdir())\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[T.Tensor, np.ndarray, str]:\n",
        "        path = str(self.files[index % len(self.files)])\n",
        "        img = np.array(Image.open(path))\n",
        "\n",
        "        pad = ((24, 24), (0, 0), (0, 0))\n",
        "\n",
        "        # img = np.pad(img, pad, 'constant', constant_values=0) / 255\n",
        "        img = np.pad(img, pad, mode=\"edge\") / 255.0\n",
        "\n",
        "        img = np.transpose(img, (2, 0, 1))\n",
        "        img = T.from_numpy(img).float()\n",
        "\n",
        "        patches = np.reshape(img, (3, 6, 128, 10, 128))\n",
        "        patches = np.transpose(patches, (0, 1, 3, 2, 4))\n",
        "\n",
        "        return img, patches, path\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# namespace.py\n",
        "import json\n",
        "\n",
        "\n",
        "class Namespace:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update({k: self.__elt(v) for k, v in kwargs.items()})\n",
        "\n",
        "    def __elt(self, xs):\n",
        "        if isinstance(xs, dict):\n",
        "            return Namespace(**xs)\n",
        "\n",
        "        if isinstance(xs, (list, tuple)):\n",
        "            return [self.__elt(x) for x in xs]\n",
        "\n",
        "        return xs\n",
        "\n",
        "    def __str__(self):\n",
        "        return json.dumps(self.__get_nested(), indent=4)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.__dict__)\n",
        "\n",
        "    def __get_nested(self) -> dict:\n",
        "        out = {}\n",
        "\n",
        "        for k, v in self.__dict__.items():\n",
        "            # nested\n",
        "            if isinstance(v, Namespace):\n",
        "                out[k] = \"<self>\" if v is self else Namespace.__get_nested(v)\n",
        "\n",
        "            # non-primitive type, call its str method\n",
        "            elif hasattr(v, \"__dict__\"):\n",
        "                out[k] = str(v)\n",
        "\n",
        "            # primitives\n",
        "            else:\n",
        "                out[k] = v\n",
        "\n",
        "        return out\n",
        "\n",
        "    def is_empty(self):\n",
        "        return len(self) == 0\n",
        "\n",
        "    def to_dict(self) -> dict:\n",
        "        return self.__get_nested()\n",
        "\n",
        "    def to_file(self, fname) -> None:\n",
        "        with open(fname, \"wt\") as fp:\n",
        "            fp.write(str(self))"
      ],
      "metadata": {
        "id": "F4wWVEbx0MBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdbP3aerazYR",
        "outputId": "58550460-922b-4f48-bcc9-52315e849cc1"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# train\n",
        "\n",
        "import os\n",
        "import yaml\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "# from utils import save_imgs\n",
        "\n",
        "# from namespace import Namespace\n",
        "# from logger import Logger\n",
        "\n",
        "from models.cae_32x32x32_zero_pad_bin import CAE\n",
        "\n",
        "logger = Logger(__name__, colorize=True)\n",
        "\n",
        "device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def train(cfg):\n",
        "    #assert cfg.device == \"cpu\" or (cfg.device == \"cuda\" and T.cuda.is_available())\n",
        "\n",
        "    root_dir = os.path.dirname(os.getcwd())\n",
        "\n",
        "    logger.info(\"training: experiment %s\" % (cfg.exp_name))\n",
        "\n",
        "    # make dir-tree\n",
        "    exp_dir = root_dir+\"/experiments/\"+cfg.exp_name\n",
        "\n",
        "    for d in [\"out\", \"checkpoint\", \"logs\"]:\n",
        "        os.makedirs(exp_dir+\"/\"+d, exist_ok=True)\n",
        "\n",
        "    cfg.to_file(exp_dir+\"/\"+\"train_config.json\")\n",
        "\n",
        "    # tb tb_writer\n",
        "    tb_writer = SummaryWriter(exp_dir+\"/\"+\"logs\")\n",
        "    logger.info(\"started tensorboard writer\")\n",
        "\n",
        "    model = CAE()\n",
        "    model.train()\n",
        "    if cfg.device == \"cuda\":\n",
        "        model.cuda()\n",
        "    logger.info(f\"loaded model on {cfg.device}\")\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset=ImageFolder720p(cfg.dataset_path),\n",
        "        batch_size=cfg.batch_size,\n",
        "        shuffle=cfg.shuffle,\n",
        "        num_workers=cfg.num_workers,\n",
        "    )\n",
        "    logger.info(f\"loaded dataset from {cfg.dataset_path}\")\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate, weight_decay=1e-5)\n",
        "    loss_criterion = nn.MSELoss()\n",
        "\n",
        "    avg_loss, epoch_avg = 0.0, 0.0\n",
        "    ts = 0\n",
        "\n",
        "    # EPOCHS\n",
        "    for epoch_idx in range(cfg.start_epoch, cfg.num_epochs + 1):\n",
        "        # BATCHES\n",
        "        for batch_idx, data in enumerate(dataloader, start=1):\n",
        "            img, patches, _ = data\n",
        "\n",
        "            if cfg.device == \"cuda\":\n",
        "                patches = patches.cuda()\n",
        "\n",
        "            avg_loss_per_image = 0.0\n",
        "            for i in range(6):\n",
        "                for j in range(10):\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    x = patches[:, :, i, j, :, :]\n",
        "                    y = model(x)\n",
        "                    loss = loss_criterion(y, x)\n",
        "\n",
        "                    avg_loss_per_image += (1 / 60) * loss.item()\n",
        "\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            avg_loss += avg_loss_per_image\n",
        "            epoch_avg += avg_loss_per_image\n",
        "\n",
        "            if batch_idx % cfg.batch_every == 0:\n",
        "                tb_writer.add_scalar(\"train/avg_loss\", avg_loss / cfg.batch_every, ts)\n",
        "\n",
        "                for name, param in model.named_parameters():\n",
        "                    tb_writer.add_histogram(name, param, ts)\n",
        "\n",
        "                logger.debug(\n",
        "                    \"[%3d/%3d][%5d/%5d] avg_loss: %.8f\"\n",
        "                    % (\n",
        "                        epoch_idx,\n",
        "                        cfg.num_epochs,\n",
        "                        batch_idx,\n",
        "                        len(dataloader),\n",
        "                        avg_loss / cfg.batch_every,\n",
        "                    )\n",
        "                )\n",
        "\n",
        "                avg_loss = 0.0\n",
        "                ts += 1\n",
        "            # -- end batch every\n",
        "\n",
        "            if batch_idx % cfg.save_every == 0:\n",
        "                out = T.zeros(6, 10, 3, 128, 128)\n",
        "                for i in range(6):\n",
        "                    for j in range(10):\n",
        "                        x = patches[0, :, i, j, :, :].unsqueeze(0).cuda()\n",
        "                        out[i, j] = model(x).cpu().data\n",
        "\n",
        "                out = np.transpose(out, (0, 3, 1, 4, 2))\n",
        "                out = np.reshape(out, (768, 1280, 3))\n",
        "                out = np.transpose(out, (2, 0, 1))\n",
        "\n",
        "                y = T.cat((img[0], out), dim=2).unsqueeze(0)\n",
        "                save_imgs(\n",
        "                    imgs=y,\n",
        "                    to_size=(3, 768, 2 * 1280),\n",
        "                    name=exp_dir+\"/\"+f\"out/{epoch_idx}_{batch_idx}.png\",\n",
        "                )\n",
        "            # -- end save every\n",
        "        # -- end batches\n",
        "\n",
        "        if epoch_idx % cfg.epoch_every == 0:\n",
        "            epoch_avg /= len(dataloader) * cfg.epoch_every\n",
        "\n",
        "            tb_writer.add_scalar(\n",
        "                \"train/epoch_avg_loss\",\n",
        "                avg_loss / cfg.batch_every,\n",
        "                epoch_idx // cfg.epoch_every,\n",
        "            )\n",
        "\n",
        "            logger.info(\"Epoch avg = %.8f\" % epoch_avg)\n",
        "            epoch_avg = 0.0\n",
        "\n",
        "            T.save(model.state_dict(), exp_dir +\"/\"+ f\"checkpoint/model_{epoch_idx}.pth\")\n",
        "        # -- end epoch every\n",
        "    # -- end epoch\n",
        "\n",
        "    # save final model\n",
        "    T.save(model.state_dict(), exp_dir +\"/\"+ \"model_final.pth\")\n",
        "\n",
        "    # cleaning\n",
        "    tb_writer.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    #parser = argparse.ArgumentParser()\n",
        "    #parser.add_argument('-f')\n",
        "    #parser.add_argument(\"--config\", type=str, required=True)\n",
        "    #args = parser.parse_args()\n",
        "\n",
        "    with open(\"configs/train.yaml\", \"rt\") as fp:\n",
        "        cfg = Namespace(**yaml.safe_load(fp))\n",
        "        print(cfg)\n",
        "\n",
        "    train(cfg)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"num_epochs\": 1,\n",
            "    \"batch_size\": 16,\n",
            "    \"learning_rate\": 0.0001,\n",
            "    \"resume\": false,\n",
            "    \"checkpoint\": null,\n",
            "    \"start_epoch\": 1,\n",
            "    \"exp_name\": \"training\",\n",
            "    \"batch_every\": 1,\n",
            "    \"save_every\": 10,\n",
            "    \"epoch_every\": 1,\n",
            "    \"shuffle\": true,\n",
            "    \"dataset_path\": \"drive/MyDrive/yt_small_720p\",\n",
            "    \"num_workers\": 2,\n",
            "    \"device\": \"cuda\"\n",
            "}\n",
            "[73][21-11-22 13:12:21.483 @ train] INFO: training: experiment training\n",
            "[73][21-11-22 13:12:21.504 @ train] INFO: started tensorboard writer\n",
            "[73][21-11-22 13:12:21.539 @ train] INFO: loaded model on cuda\n",
            "[73][21-11-22 13:12:21.589 @ train] INFO: loaded dataset from drive/MyDrive/yt_small_720p\n",
            "[73][21-11-22 13:12:56.464 @ train] DEBUG: [  1/  1][    1/  143] avg_loss: 0.15425662\n",
            "[73][21-11-22 13:13:28.992 @ train] DEBUG: [  1/  1][    2/  143] avg_loss: 0.02007326\n",
            "[73][21-11-22 13:14:01.441 @ train] DEBUG: [  1/  1][    3/  143] avg_loss: 0.01710654\n",
            "[73][21-11-22 13:14:33.798 @ train] DEBUG: [  1/  1][    4/  143] avg_loss: 0.01456667\n",
            "[73][21-11-22 13:15:06.170 @ train] DEBUG: [  1/  1][    5/  143] avg_loss: 0.00998596\n",
            "[73][21-11-22 13:15:38.568 @ train] DEBUG: [  1/  1][    6/  143] avg_loss: 0.01419734\n",
            "[73][21-11-22 13:16:10.908 @ train] DEBUG: [  1/  1][    7/  143] avg_loss: 0.00966432\n",
            "[73][21-11-22 13:16:43.316 @ train] DEBUG: [  1/  1][    8/  143] avg_loss: 0.00861355\n",
            "[73][21-11-22 13:17:15.772 @ train] DEBUG: [  1/  1][    9/  143] avg_loss: 0.00840443\n",
            "[73][21-11-22 13:17:48.184 @ train] DEBUG: [  1/  1][   10/  143] avg_loss: 0.00789915\n",
            "[73][21-11-22 13:18:26.046 @ train] DEBUG: [  1/  1][   11/  143] avg_loss: 0.00914565\n",
            "[73][21-11-22 13:18:58.487 @ train] DEBUG: [  1/  1][   12/  143] avg_loss: 0.00746166\n",
            "[73][21-11-22 13:19:30.957 @ train] DEBUG: [  1/  1][   13/  143] avg_loss: 0.00850990\n",
            "[73][21-11-22 13:20:03.389 @ train] DEBUG: [  1/  1][   14/  143] avg_loss: 0.00533333\n",
            "[73][21-11-22 13:20:35.888 @ train] DEBUG: [  1/  1][   15/  143] avg_loss: 0.01059927\n",
            "[73][21-11-22 13:21:08.314 @ train] DEBUG: [  1/  1][   16/  143] avg_loss: 0.00766533\n",
            "[73][21-11-22 13:21:40.673 @ train] DEBUG: [  1/  1][   17/  143] avg_loss: 0.01086856\n",
            "[73][21-11-22 13:22:13.058 @ train] DEBUG: [  1/  1][   18/  143] avg_loss: 0.01062382\n",
            "[73][21-11-22 13:22:45.453 @ train] DEBUG: [  1/  1][   19/  143] avg_loss: 0.00751160\n",
            "[73][21-11-22 13:23:17.849 @ train] DEBUG: [  1/  1][   20/  143] avg_loss: 0.00887716\n",
            "[73][21-11-22 13:23:55.419 @ train] DEBUG: [  1/  1][   21/  143] avg_loss: 0.00767623\n",
            "[73][21-11-22 13:24:27.905 @ train] DEBUG: [  1/  1][   22/  143] avg_loss: 0.00858236\n",
            "[73][21-11-22 13:25:00.440 @ train] DEBUG: [  1/  1][   23/  143] avg_loss: 0.01045016\n",
            "[73][21-11-22 13:25:32.833 @ train] DEBUG: [  1/  1][   24/  143] avg_loss: 0.01007952\n",
            "[73][21-11-22 13:26:05.061 @ train] DEBUG: [  1/  1][   25/  143] avg_loss: 0.00744887\n",
            "[73][21-11-22 13:26:37.271 @ train] DEBUG: [  1/  1][   26/  143] avg_loss: 0.00821018\n",
            "[73][21-11-22 13:27:09.523 @ train] DEBUG: [  1/  1][   27/  143] avg_loss: 0.00905304\n",
            "[73][21-11-22 13:27:41.763 @ train] DEBUG: [  1/  1][   28/  143] avg_loss: 0.01087713\n",
            "[73][21-11-22 13:28:13.954 @ train] DEBUG: [  1/  1][   29/  143] avg_loss: 0.00486374\n",
            "[73][21-11-22 13:28:46.113 @ train] DEBUG: [  1/  1][   30/  143] avg_loss: 0.00952450\n",
            "[73][21-11-22 13:29:23.493 @ train] DEBUG: [  1/  1][   31/  143] avg_loss: 0.00688702\n",
            "[73][21-11-22 13:29:55.657 @ train] DEBUG: [  1/  1][   32/  143] avg_loss: 0.00642575\n",
            "[73][21-11-22 13:30:27.871 @ train] DEBUG: [  1/  1][   33/  143] avg_loss: 0.00772931\n",
            "[73][21-11-22 13:31:00.104 @ train] DEBUG: [  1/  1][   34/  143] avg_loss: 0.00756673\n",
            "[73][21-11-22 13:31:32.355 @ train] DEBUG: [  1/  1][   35/  143] avg_loss: 0.00726081\n",
            "[73][21-11-22 13:32:04.604 @ train] DEBUG: [  1/  1][   36/  143] avg_loss: 0.00545663\n",
            "[73][21-11-22 13:32:36.904 @ train] DEBUG: [  1/  1][   37/  143] avg_loss: 0.00834459\n",
            "[73][21-11-22 13:33:09.209 @ train] DEBUG: [  1/  1][   38/  143] avg_loss: 0.01075618\n",
            "[73][21-11-22 13:33:41.444 @ train] DEBUG: [  1/  1][   39/  143] avg_loss: 0.00654639\n",
            "[73][21-11-22 13:34:13.709 @ train] DEBUG: [  1/  1][   40/  143] avg_loss: 0.00640391\n",
            "[73][21-11-22 13:34:50.983 @ train] DEBUG: [  1/  1][   41/  143] avg_loss: 0.00621474\n",
            "[73][21-11-22 13:35:23.224 @ train] DEBUG: [  1/  1][   42/  143] avg_loss: 0.00902161\n",
            "[73][21-11-22 13:35:55.396 @ train] DEBUG: [  1/  1][   43/  143] avg_loss: 0.00939790\n",
            "[73][21-11-22 13:36:27.547 @ train] DEBUG: [  1/  1][   44/  143] avg_loss: 0.01182795\n",
            "[73][21-11-22 13:36:59.661 @ train] DEBUG: [  1/  1][   45/  143] avg_loss: 0.01091754\n",
            "[73][21-11-22 13:37:31.872 @ train] DEBUG: [  1/  1][   46/  143] avg_loss: 0.00777543\n",
            "[73][21-11-22 13:38:04.201 @ train] DEBUG: [  1/  1][   47/  143] avg_loss: 0.00533107\n",
            "[73][21-11-22 13:38:36.462 @ train] DEBUG: [  1/  1][   48/  143] avg_loss: 0.00821383\n",
            "[73][21-11-22 13:39:08.705 @ train] DEBUG: [  1/  1][   49/  143] avg_loss: 0.00872789\n",
            "[73][21-11-22 13:39:40.965 @ train] DEBUG: [  1/  1][   50/  143] avg_loss: 0.00576791\n",
            "[73][21-11-22 13:40:18.634 @ train] DEBUG: [  1/  1][   51/  143] avg_loss: 0.00370261\n",
            "[73][21-11-22 13:40:50.923 @ train] DEBUG: [  1/  1][   52/  143] avg_loss: 0.00361579\n",
            "[73][21-11-22 13:41:23.171 @ train] DEBUG: [  1/  1][   53/  143] avg_loss: 0.00322071\n",
            "[73][21-11-22 13:41:55.472 @ train] DEBUG: [  1/  1][   54/  143] avg_loss: 0.00366457\n",
            "[73][21-11-22 13:42:27.723 @ train] DEBUG: [  1/  1][   55/  143] avg_loss: 0.00238869\n",
            "[73][21-11-22 13:42:59.904 @ train] DEBUG: [  1/  1][   56/  143] avg_loss: 0.00278873\n",
            "[73][21-11-22 13:43:32.129 @ train] DEBUG: [  1/  1][   57/  143] avg_loss: 0.00258518\n",
            "[73][21-11-22 13:44:04.280 @ train] DEBUG: [  1/  1][   58/  143] avg_loss: 0.00215954\n",
            "[73][21-11-22 13:44:36.520 @ train] DEBUG: [  1/  1][   59/  143] avg_loss: 0.00291355\n",
            "[73][21-11-22 13:45:08.811 @ train] DEBUG: [  1/  1][   60/  143] avg_loss: 0.00200827\n",
            "[73][21-11-22 13:45:46.211 @ train] DEBUG: [  1/  1][   61/  143] avg_loss: 0.00308450\n",
            "[73][21-11-22 13:46:18.483 @ train] DEBUG: [  1/  1][   62/  143] avg_loss: 0.00266607\n",
            "[73][21-11-22 13:46:50.749 @ train] DEBUG: [  1/  1][   63/  143] avg_loss: 0.00597387\n",
            "[73][21-11-22 13:47:23.044 @ train] DEBUG: [  1/  1][   64/  143] avg_loss: 0.00278201\n",
            "[73][21-11-22 13:47:55.348 @ train] DEBUG: [  1/  1][   65/  143] avg_loss: 0.00378055\n",
            "[73][21-11-22 13:48:27.675 @ train] DEBUG: [  1/  1][   66/  143] avg_loss: 0.00318701\n",
            "[73][21-11-22 13:48:59.911 @ train] DEBUG: [  1/  1][   67/  143] avg_loss: 0.00282997\n",
            "[73][21-11-22 13:49:32.183 @ train] DEBUG: [  1/  1][   68/  143] avg_loss: 0.00225700\n",
            "[73][21-11-22 13:50:04.408 @ train] DEBUG: [  1/  1][   69/  143] avg_loss: 0.00293040\n",
            "[73][21-11-22 13:50:36.611 @ train] DEBUG: [  1/  1][   70/  143] avg_loss: 0.00303892\n",
            "[73][21-11-22 13:51:14.015 @ train] DEBUG: [  1/  1][   71/  143] avg_loss: 0.00259489\n",
            "[73][21-11-22 13:51:46.236 @ train] DEBUG: [  1/  1][   72/  143] avg_loss: 0.00248833\n",
            "[73][21-11-22 13:52:18.468 @ train] DEBUG: [  1/  1][   73/  143] avg_loss: 0.00181495\n",
            "[73][21-11-22 13:52:50.648 @ train] DEBUG: [  1/  1][   74/  143] avg_loss: 0.00212687\n",
            "[73][21-11-22 13:53:22.899 @ train] DEBUG: [  1/  1][   75/  143] avg_loss: 0.00205079\n",
            "[73][21-11-22 13:53:55.108 @ train] DEBUG: [  1/  1][   76/  143] avg_loss: 0.00237246\n",
            "[73][21-11-22 13:54:27.336 @ train] DEBUG: [  1/  1][   77/  143] avg_loss: 0.00139953\n",
            "[73][21-11-22 13:54:59.520 @ train] DEBUG: [  1/  1][   78/  143] avg_loss: 0.00202228\n",
            "[73][21-11-22 13:55:31.695 @ train] DEBUG: [  1/  1][   79/  143] avg_loss: 0.00242757\n",
            "[73][21-11-22 13:56:03.836 @ train] DEBUG: [  1/  1][   80/  143] avg_loss: 0.00175858\n",
            "[73][21-11-22 13:56:41.127 @ train] DEBUG: [  1/  1][   81/  143] avg_loss: 0.00243303\n",
            "[73][21-11-22 13:57:13.273 @ train] DEBUG: [  1/  1][   82/  143] avg_loss: 0.00200470\n",
            "[73][21-11-22 13:57:45.418 @ train] DEBUG: [  1/  1][   83/  143] avg_loss: 0.00219877\n",
            "[73][21-11-22 13:58:17.588 @ train] DEBUG: [  1/  1][   84/  143] avg_loss: 0.00307442\n",
            "[73][21-11-22 13:58:49.739 @ train] DEBUG: [  1/  1][   85/  143] avg_loss: 0.00293472\n",
            "[73][21-11-22 13:59:21.910 @ train] DEBUG: [  1/  1][   86/  143] avg_loss: 0.00198929\n",
            "[73][21-11-22 13:59:54.047 @ train] DEBUG: [  1/  1][   87/  143] avg_loss: 0.00204103\n",
            "[73][21-11-22 14:00:26.208 @ train] DEBUG: [  1/  1][   88/  143] avg_loss: 0.00141416\n",
            "[73][21-11-22 14:00:58.369 @ train] DEBUG: [  1/  1][   89/  143] avg_loss: 0.00216389\n",
            "[73][21-11-22 14:01:30.551 @ train] DEBUG: [  1/  1][   90/  143] avg_loss: 0.00183074\n",
            "[73][21-11-22 14:02:07.809 @ train] DEBUG: [  1/  1][   91/  143] avg_loss: 0.00243352\n",
            "[73][21-11-22 14:02:39.925 @ train] DEBUG: [  1/  1][   92/  143] avg_loss: 0.00200151\n",
            "[73][21-11-22 14:03:12.062 @ train] DEBUG: [  1/  1][   93/  143] avg_loss: 0.00331071\n",
            "[73][21-11-22 14:03:44.197 @ train] DEBUG: [  1/  1][   94/  143] avg_loss: 0.00207500\n",
            "[73][21-11-22 14:04:16.331 @ train] DEBUG: [  1/  1][   95/  143] avg_loss: 0.00176815\n",
            "[73][21-11-22 14:04:48.512 @ train] DEBUG: [  1/  1][   96/  143] avg_loss: 0.00216326\n",
            "[73][21-11-22 14:05:20.667 @ train] DEBUG: [  1/  1][   97/  143] avg_loss: 0.00180118\n",
            "[73][21-11-22 14:05:52.812 @ train] DEBUG: [  1/  1][   98/  143] avg_loss: 0.00199532\n",
            "[73][21-11-22 14:06:24.942 @ train] DEBUG: [  1/  1][   99/  143] avg_loss: 0.00240406\n",
            "[73][21-11-22 14:06:57.111 @ train] DEBUG: [  1/  1][  100/  143] avg_loss: 0.00337209\n",
            "[73][21-11-22 14:07:34.498 @ train] DEBUG: [  1/  1][  101/  143] avg_loss: 0.00360427\n",
            "[73][21-11-22 14:08:06.646 @ train] DEBUG: [  1/  1][  102/  143] avg_loss: 0.00222107\n",
            "[73][21-11-22 14:08:38.837 @ train] DEBUG: [  1/  1][  103/  143] avg_loss: 0.00248967\n",
            "[73][21-11-22 14:09:10.957 @ train] DEBUG: [  1/  1][  104/  143] avg_loss: 0.00216132\n",
            "[73][21-11-22 14:09:43.127 @ train] DEBUG: [  1/  1][  105/  143] avg_loss: 0.00168466\n",
            "[73][21-11-22 14:10:15.284 @ train] DEBUG: [  1/  1][  106/  143] avg_loss: 0.00190031\n",
            "[73][21-11-22 14:10:47.488 @ train] DEBUG: [  1/  1][  107/  143] avg_loss: 0.00309345\n",
            "[73][21-11-22 14:11:19.648 @ train] DEBUG: [  1/  1][  108/  143] avg_loss: 0.00193626\n",
            "[73][21-11-22 14:11:51.836 @ train] DEBUG: [  1/  1][  109/  143] avg_loss: 0.00206049\n",
            "[73][21-11-22 14:12:23.993 @ train] DEBUG: [  1/  1][  110/  143] avg_loss: 0.00174472\n",
            "[73][21-11-22 14:13:01.444 @ train] DEBUG: [  1/  1][  111/  143] avg_loss: 0.00150556\n",
            "[73][21-11-22 14:13:33.578 @ train] DEBUG: [  1/  1][  112/  143] avg_loss: 0.00175138\n",
            "[73][21-11-22 14:14:05.747 @ train] DEBUG: [  1/  1][  113/  143] avg_loss: 0.00212921\n",
            "[73][21-11-22 14:14:37.922 @ train] DEBUG: [  1/  1][  114/  143] avg_loss: 0.00248853\n",
            "[73][21-11-22 14:15:10.097 @ train] DEBUG: [  1/  1][  115/  143] avg_loss: 0.00155119\n",
            "[73][21-11-22 14:15:42.302 @ train] DEBUG: [  1/  1][  116/  143] avg_loss: 0.00177684\n",
            "[73][21-11-22 14:16:14.447 @ train] DEBUG: [  1/  1][  117/  143] avg_loss: 0.00291620\n",
            "[73][21-11-22 14:16:46.604 @ train] DEBUG: [  1/  1][  118/  143] avg_loss: 0.00217076\n",
            "[73][21-11-22 14:17:18.784 @ train] DEBUG: [  1/  1][  119/  143] avg_loss: 0.00381475\n",
            "[73][21-11-22 14:17:50.972 @ train] DEBUG: [  1/  1][  120/  143] avg_loss: 0.00224017\n",
            "[73][21-11-22 14:18:28.281 @ train] DEBUG: [  1/  1][  121/  143] avg_loss: 0.00246295\n",
            "[73][21-11-22 14:19:00.433 @ train] DEBUG: [  1/  1][  122/  143] avg_loss: 0.00225798\n",
            "[73][21-11-22 14:19:32.493 @ train] DEBUG: [  1/  1][  123/  143] avg_loss: 0.00197011\n",
            "[73][21-11-22 14:20:04.507 @ train] DEBUG: [  1/  1][  124/  143] avg_loss: 0.00196141\n",
            "[73][21-11-22 14:20:36.478 @ train] DEBUG: [  1/  1][  125/  143] avg_loss: 0.00302661\n",
            "[73][21-11-22 14:21:08.423 @ train] DEBUG: [  1/  1][  126/  143] avg_loss: 0.00259122\n",
            "[73][21-11-22 14:21:40.384 @ train] DEBUG: [  1/  1][  127/  143] avg_loss: 0.00166852\n",
            "[73][21-11-22 14:22:12.318 @ train] DEBUG: [  1/  1][  128/  143] avg_loss: 0.00200583\n",
            "[73][21-11-22 14:22:44.222 @ train] DEBUG: [  1/  1][  129/  143] avg_loss: 0.00201453\n",
            "[73][21-11-22 14:23:16.186 @ train] DEBUG: [  1/  1][  130/  143] avg_loss: 0.00245662\n",
            "[73][21-11-22 14:23:53.388 @ train] DEBUG: [  1/  1][  131/  143] avg_loss: 0.00186891\n",
            "[73][21-11-22 14:24:25.387 @ train] DEBUG: [  1/  1][  132/  143] avg_loss: 0.00189803\n",
            "[73][21-11-22 14:24:57.387 @ train] DEBUG: [  1/  1][  133/  143] avg_loss: 0.00126067\n",
            "[73][21-11-22 14:25:29.356 @ train] DEBUG: [  1/  1][  134/  143] avg_loss: 0.00641453\n",
            "[73][21-11-22 14:26:01.348 @ train] DEBUG: [  1/  1][  135/  143] avg_loss: 0.00221171\n",
            "[73][21-11-22 14:26:33.349 @ train] DEBUG: [  1/  1][  136/  143] avg_loss: 0.00188254\n",
            "[73][21-11-22 14:27:05.294 @ train] DEBUG: [  1/  1][  137/  143] avg_loss: 0.00208071\n",
            "[73][21-11-22 14:27:37.278 @ train] DEBUG: [  1/  1][  138/  143] avg_loss: 0.00336458\n",
            "[73][21-11-22 14:28:09.279 @ train] DEBUG: [  1/  1][  139/  143] avg_loss: 0.00290270\n",
            "[73][21-11-22 14:28:41.243 @ train] DEBUG: [  1/  1][  140/  143] avg_loss: 0.00136990\n",
            "[73][21-11-22 14:29:18.358 @ train] DEBUG: [  1/  1][  141/  143] avg_loss: 0.00200806\n",
            "[73][21-11-22 14:29:50.317 @ train] DEBUG: [  1/  1][  142/  143] avg_loss: 0.00207745\n",
            "[73][21-11-22 14:30:21.105 @ train] DEBUG: [  1/  1][  143/  143] avg_loss: 0.00151820\n",
            "[73][21-11-22 14:30:21.215 @ train] INFO: Epoch avg = 0.00574309\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jwsYIzXR7Ak",
        "outputId": "a97e3dda-24fa-40ba-8318-7586cee57855"
      },
      "source": [
        "# test.py\n",
        "import os\n",
        "import yaml\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "#from torch.utils.data import DataLoader\n",
        "\n",
        "#from data_loader import ImageFolder720p\n",
        "#from utils import save_imgs\n",
        "\n",
        "# from namespace import Namespace\n",
        "# from logger import Logger\n",
        "\n",
        "from models.cae_32x32x32_zero_pad_bin import CAE\n",
        "\n",
        "ROOT_EXP_DIR = os.path.dirname(os.getcwd())+\"/\"+ \"experiments\"\n",
        "\n",
        "logger = Logger(__name__, colorize=True)\n",
        "\n",
        "\n",
        "def test(cfg):\n",
        "    assert cfg.checkpoint not in [None, \"\"]\n",
        "    assert cfg.device == \"cpu\" or (cfg.device == \"cuda\" and T.cuda.is_available())\n",
        "\n",
        "    exp_dir = ROOT_EXP_DIR +\"/\"+ cfg.exp_name\n",
        "    os.makedirs(exp_dir +\"/\"+ \"out\", exist_ok=True)\n",
        "    cfg.to_file(exp_dir +\"/\"+ \"test_config.json\")\n",
        "    logger.info(f\"[exp dir={exp_dir}]\")\n",
        "\n",
        "    model = CAE()\n",
        "    model.load_state_dict(T.load(cfg.checkpoint))\n",
        "    model.eval()\n",
        "    if cfg.device == \"cuda\":\n",
        "        model.cuda()\n",
        "    logger.info(f\"[model={cfg.checkpoint}] on {cfg.device}\")\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset=ImageFolder720p(cfg.dataset_path), batch_size=1, shuffle=cfg.shuffle\n",
        "    )\n",
        "    logger.info(f\"[dataset={cfg.dataset_path}]\")\n",
        "\n",
        "    loss_criterion = nn.MSELoss()\n",
        "\n",
        "    for batch_idx, data in enumerate(dataloader, start=1):\n",
        "        img, patches, _ = data\n",
        "        if cfg.device == \"cuda\":\n",
        "            patches = patches.cuda()\n",
        "\n",
        "        if batch_idx % cfg.batch_every == 0:\n",
        "            pass\n",
        "\n",
        "        out = T.zeros(6, 10, 3, 128, 128)\n",
        "        avg_loss = 0\n",
        "\n",
        "        for i in range(6):\n",
        "            for j in range(10):\n",
        "                x = patches[:, :, i, j, :, :].cuda()\n",
        "                y = model(x)\n",
        "                out[i, j] = y.data\n",
        "\n",
        "                loss = loss_criterion(y, x)\n",
        "                avg_loss += (1 / 60) * loss.item()\n",
        "\n",
        "        logger.debug(\"[%5d/%5d] avg_loss: %f\", batch_idx, len(dataloader), avg_loss)\n",
        "\n",
        "        # save output\n",
        "        out = np.transpose(out, (0, 3, 1, 4, 2))\n",
        "        out = np.reshape(out, (768, 1280, 3))\n",
        "        out = np.transpose(out, (2, 0, 1))\n",
        "\n",
        "        y = T.cat((img[0], out), dim=2)\n",
        "        save_imgs(\n",
        "            imgs=y.unsqueeze(0),\n",
        "            to_size=(3, 768, 2 * 1280),\n",
        "            name=exp_dir +\"/\"+ f\"out/test_{batch_idx}.png\",\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    with open(\"configs/test.yaml\", \"rt\") as fp:\n",
        "        cfg = Namespace(**yaml.safe_load(fp))\n",
        "        print(cfg)\n",
        "\n",
        "    test(cfg)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"checkpoint\": \"../experiments/training/model_final.pth\",\n",
            "    \"exp_name\": \"testing\",\n",
            "    \"batch_every\": 100,\n",
            "    \"shuffle\": false,\n",
            "    \"dataset_path\": \"testing\",\n",
            "    \"num_workers\": 1,\n",
            "    \"device\": \"cuda\"\n",
            "}\n",
            "[73][21-11-22 15:11:45.776 @ test] INFO: [exp dir=//experiments/testing]\n",
            "[73][21-11-22 15:11:45.836 @ test] INFO: [model=../experiments/training/model_final.pth] on cuda\n",
            "[73][21-11-22 15:11:45.842 @ test] INFO: [dataset=testing]\n",
            "[73][21-11-22 15:11:50.505 @ test] DEBUG: [    1/    1] avg_loss: 0.013746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SSIM\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image \n",
        "from scipy.signal import convolve2d\n",
        " \n",
        "def matlab_style_gauss2D(shape=(3,3),sigma=0.5):\n",
        "    \"\"\"\n",
        "    2D gaussian mask - should give the same result as MATLAB's\n",
        "    fspecial('gaussian',[shape],[sigma])\n",
        "    \"\"\"\n",
        "    m,n = [(ss-1.)/2. for ss in shape]\n",
        "    y,x = np.ogrid[-m:m+1,-n:n+1]\n",
        "    h = np.exp( -(x*x + y*y) / (2.*sigma*sigma) )\n",
        "    h[ h < np.finfo(h.dtype).eps*h.max() ] = 0\n",
        "    sumh = h.sum()\n",
        "    if sumh != 0:\n",
        "        h /= sumh\n",
        "    return h\n",
        " \n",
        "def filter2(x, kernel, mode='same'):\n",
        "    return convolve2d(x, np.rot90(kernel, 2), mode=mode)\n",
        " \n",
        "def compute_ssim(im1, im2, k1=0.01, k2=0.03, win_size=11, L=255):\n",
        " \n",
        "    if not im1.shape == im2.shape:\n",
        "        raise ValueError(\"Input Imagees must have the same dimensions\")\n",
        "    if len(im1.shape) > 2:\n",
        "        raise ValueError(\"Please input the images with 1 channel\")\n",
        " \n",
        "    M, N = im1.shape\n",
        "    C1 = (k1*L)**2\n",
        "    C2 = (k2*L)**2\n",
        "    window = matlab_style_gauss2D(shape=(win_size,win_size), sigma=1.5)\n",
        "    window = window/np.sum(np.sum(window))\n",
        " \n",
        "    if im1.dtype == np.uint8:\n",
        "        im1 = np.double(im1)\n",
        "    if im2.dtype == np.uint8:\n",
        "        im2 = np.double(im2)\n",
        " \n",
        "    mu1 = filter2(im1, window, 'valid')\n",
        "    mu2 = filter2(im2, window, 'valid')\n",
        "    mu1_sq = mu1 * mu1\n",
        "    mu2_sq = mu2 * mu2\n",
        "    mu1_mu2 = mu1 * mu2\n",
        "    sigma1_sq = filter2(im1*im1, window, 'valid') - mu1_sq\n",
        "    sigma2_sq = filter2(im2*im2, window, 'valid') - mu2_sq\n",
        "    sigmal2 = filter2(im1*im2, window, 'valid') - mu1_mu2\n",
        " \n",
        "    ssim_map = ((2*mu1_mu2+C1) * (2*sigmal2+C2)) / ((mu1_sq+mu2_sq+C1) * (sigma1_sq+sigma2_sq+C2))\n",
        " \n",
        "    return np.mean(np.mean(ssim_map))\n",
        " \n",
        " \n",
        "if __name__ == \"__main__\":\n",
        "    im1= cv2.imread(\"1.png\")\n",
        "    im1= cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
        "    im2= cv2.imread(\"2.png\")\n",
        "    im2= cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
        " \n",
        "    print(compute_ssim(np.array(im1),np.array(im2)))"
      ],
      "metadata": {
        "id": "JzveAfKQ0cDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PSNR\n",
        "import cv2\n",
        "import numpy as np\n",
        "import math\n",
        " \n",
        "\n",
        " \n",
        "def psnr(img1, img2):\n",
        "   mse = np.mean( (img1/255. - img2/255.) ** 2 )\n",
        "   if mse < 1.0e-10:\n",
        "      return 100\n",
        "   PIXEL_MAX = 1\n",
        "   return 20 * math.log10(PIXEL_MAX / math.sqrt(mse))\n",
        "\n",
        "\n",
        "\n",
        "im1= cv2.imread(\"1.png\")\n",
        "im2= cv2.imread(\"2.png\")\n",
        "print(psnr(im1,im2))"
      ],
      "metadata": {
        "id": "k4mzPxIb0lGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MS-SSIM\n",
        "#ssim.py\n",
        "import numpy as np\n",
        "import scipy.signal\n",
        "import cv2\n",
        "\n",
        "\n",
        "def ssim_index_new(img1,img2,K,win):\n",
        "\n",
        "    M,N = img1.shape\n",
        "\n",
        "    img1 = img1.astype(np.float32)\n",
        "    img2 = img2.astype(np.float32)\n",
        "\n",
        "    C1 = (K[0]*255)**2\n",
        "    C2 = (K[1]*255) ** 2\n",
        "    win = win/np.sum(win)\n",
        "\n",
        "    mu1 = scipy.signal.convolve2d(img1,win,mode='valid')\n",
        "    mu2 = scipy.signal.convolve2d(img2,win,mode='valid')\n",
        "    mu1_sq = np.multiply(mu1,mu1)\n",
        "    mu2_sq = np.multiply(mu2,mu2)\n",
        "    mu1_mu2 = np.multiply(mu1,mu2)\n",
        "    sigma1_sq = scipy.signal.convolve2d(np.multiply(img1,img1),win,mode='valid') - mu1_sq\n",
        "    sigma2_sq = scipy.signal.convolve2d(np.multiply(img2, img2), win, mode='valid') - mu2_sq\n",
        "    img12 = np.multiply(img1, img2)\n",
        "    sigma12 = scipy.signal.convolve2d(np.multiply(img1, img2), win, mode='valid') - mu1_mu2\n",
        "\n",
        "    if(C1 > 0 and C2>0):\n",
        "        ssim1 =2*sigma12 + C2\n",
        "        ssim_map = np.divide(np.multiply((2*mu1_mu2 + C1),(2*sigma12 + C2)),np.multiply((mu1_sq+mu2_sq+C1),(sigma1_sq+sigma2_sq+C2)))\n",
        "        cs_map = np.divide((2*sigma12 + C2),(sigma1_sq + sigma2_sq + C2))\n",
        "    else:\n",
        "        numerator1 = 2*mu1_mu2 + C1\n",
        "        numerator2 = 2*sigma12 + C2\n",
        "        denominator1 = mu1_sq + mu2_sq +C1\n",
        "        denominator2 = sigma1_sq + sigma2_sq +C2\n",
        "\n",
        "        ssim_map = np.ones(mu1.shape)\n",
        "        index = np.multiply(denominator1,denominator2)\n",
        "        #如果index是真，就赋值，是假就原值\n",
        "        n,m = mu1.shape\n",
        "        for i in range(n):\n",
        "            for j in range(m):\n",
        "                if(index[i][j] > 0):\n",
        "                    ssim_map[i][j] = numerator1[i][j]*numerator2[i][j]/denominator1[i][j]*denominator2[i][j]\n",
        "                else:\n",
        "                    ssim_map[i][j] = ssim_map[i][j]\n",
        "        for i in range(n):\n",
        "            for j in range(m):\n",
        "                if((denominator1[i][j] != 0)and(denominator2[i][j] == 0)):\n",
        "                    ssim_map[i][j] = numerator1[i][j]/denominator1[i][j]\n",
        "                else:\n",
        "                    ssim_map[i][j] = ssim_map[i][j]\n",
        "\n",
        "        cs_map = np.ones(mu1.shape)\n",
        "        for i in range(n):\n",
        "            for j in range(m):\n",
        "                if(denominator2[i][j] > 0):\n",
        "                    cs_map[i][j] = numerator2[i][j]/denominator2[i][j]\n",
        "                else:\n",
        "                    cs_map[i][j] = cs_map[i][j]\n",
        "\n",
        "\n",
        "    mssim = np.mean(ssim_map)\n",
        "    mcs = np.mean(cs_map)\n",
        "\n",
        "    return  mssim,mcs\n",
        "\n",
        "\n",
        "def msssim(img1,img2):\n",
        "\n",
        "    K = [0.01,0.03]\n",
        "    win  = np.multiply(cv2.getGaussianKernel(11, 1.5), (cv2.getGaussianKernel(11, 1.5)).T)  # H.shape == (r, c)\n",
        "    level = 5\n",
        "    weight = [0.0448,0.2856,0.3001,0.2363,0.1333]\n",
        "    method = 'product'\n",
        "\n",
        "    M,N = img1.shape\n",
        "    H,W = win.shape\n",
        "\n",
        "    downsample_filter = np.ones((2,2))/4\n",
        "    img1 = img1.astype(np.float32)\n",
        "    img2 = img2.astype(np.float32)\n",
        "\n",
        "    mssim_array = []\n",
        "    mcs_array = []\n",
        "\n",
        "    for i in range(0,level):\n",
        "        mssim,mcs = ssim_index_new(img1,img2,K,win)\n",
        "        mssim_array.append(mssim)\n",
        "        mcs_array.append(mcs)\n",
        "        filtered_im1 = cv2.filter2D(img1,-1,downsample_filter,anchor = (0,0),borderType=cv2.BORDER_REFLECT)\n",
        "        filtered_im2 = cv2.filter2D(img2,-1,downsample_filter,anchor = (0,0),borderType=cv2.BORDER_REFLECT)\n",
        "        img1 = filtered_im1[::2,::2]\n",
        "        img2 = filtered_im2[::2,::2]\n",
        "\n",
        "    print(np.power(mcs_array[:level-1],weight[:level-1]))\n",
        "    print(mssim_array[level-1]**weight[level-1])\n",
        "    overall_mssim = np.prod(np.power(mcs_array[:level-1],weight[:level-1]))*(mssim_array[level-1]**weight[level-1])\n",
        "    print(overall_mssim)\n",
        "    return overall_mssim\n",
        "\n",
        "\n",
        "im1= cv2.imread(\"1.png\")\n",
        "im1= cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
        "im2= cv2.imread(\"2.png\")\n",
        "im2= cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
        "print(msssim(im1,im2))\n"
      ],
      "metadata": {
        "id": "vEzjF1M00ptp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}